{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f776ca16",
   "metadata": {},
   "source": [
    "### Learning Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee75545f",
   "metadata": {},
   "source": [
    "- Reference: https://huggingface.co/docs/transformers/quicktour?installation=PyTorch\n",
    "\n",
    "- Transformers is designed to be fast and easy to use so that everyone can start learning or building with transformer models.\n",
    "\n",
    "    - The number of user-facing abstractions is limited to only THREE classes for instantiating a model, and TWO APIs for inference or training. This quickstart introduces you to Transformers’ key features and shows you how to:\n",
    "\n",
    "    - load a pretrained model\n",
    "    - run inference with Pipeline\n",
    "    - fine-tune a model with Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b95fa0",
   "metadata": {},
   "source": [
    "## PRETRAIN MODELS\n",
    "\n",
    "- Each pretrained model inherits from three base classes.\n",
    "\n",
    "#### PretrainedConfig\t\n",
    "     - A file that specifies a models attributes such as the number of attention heads or vocabulary size.\n",
    "#### PreTrainModel\n",
    "    - A model (or architecture) defined by the model attributes from the configuration file. A pretrained model only returns the raw hidden states. For a specific task, use the appropriate model head to convert the raw hidden states into a meaningful result (for example, LlamaModel versus LlamaForCausalLM).\n",
    "#### PreProcessor\n",
    "    - A class for converting raw inputs (text, images, audio, multimodal) into numerical inputs to the model. For example, PreTrainedTokenizer converts text into tensors and ImageProcessingMixin converts pixels into tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800c8d3f",
   "metadata": {},
   "source": [
    "- Use the AutoClass API to load models and preprocessors because it automatically infers the appropriate architecture for each task and machine learning framework based on the name or path to the pretrained weights and configuration file.\n",
    "\n",
    "- Use from_pretrained() to load the weights and configuration file from the Hub into the model and preprocessor class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04771943",
   "metadata": {},
   "source": [
    "### PyTorch\n",
    "\n",
    "- When you load a model, configure the following parameters to ensure the model is optimally loaded.\n",
    "\n",
    "    - device_map=\"auto\" automatically allocates the model weights to your fastest device first, which is typically the GPU.\n",
    "    - torch_dtype=\"auto\" directly initializes the model weights in the data type they’re stored in, which can help avoid loading the weights twice (PyTorch loads weights in torch.float32 by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cf6c84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.96s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "# Create an object model \n",
    "model = AutoModelForCausalLM.from_pretrained(\"./SavedModels/llama3.1-Instruct\", torch_dtype=torch.float16, device_map={\"\": \"cuda:0\"})\n",
    "# Creat an object of a tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./SavedModels/llama3.1-Instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb3f1f5",
   "metadata": {},
   "source": [
    "- Tokenize the text and return PyTorch tensors with the tokenizer. Move the model to a GPU if it’s available to accelerate inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15b959fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer([\"The secret to become successful in Robotics industry is \"], return_tensors=\"pt\").to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ae9497",
   "metadata": {},
   "source": [
    "- The model is now ready for inference or training.\n",
    "\n",
    "- For inference, pass the tokenized inputs to generate() to generate text. Decode the token ids back into text with batch_decode()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3043242a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>The secret to become successful in Robotics industry is 3 things. This is the opinion of many professionals in the field. They are:\\n1. Passion\\n2. Hard work\\n3. Networking\\nThese are the three key elements that will help you become successful in the Robotics industry. Passion will drive you to learn and improve continuously. Hard work will help you to apply your knowledge and skills to achieve your goals. Networking will help you to find opportunities, get advice and support from experienced professionals.\\nThe Robotics industry is a highly competitive field, and it requires a lot of dedication and perseverance to succeed. However, with the right mindset and approach, it is possible to achieve great things and make a meaningful impact in this field.\\nHere are some additional'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids = model.generate(**model_inputs, max_length=150) \n",
    "tokenizer.batch_decode(generated_ids)[0]\n",
    "'''\n",
    " Took 4 mins and 19 seconds to generate and output for max length 150 on my 8Gb Vram : RTX 4070 laptop\n",
    " Moreover, it consumed all of my Vram when I just load the model in Cuda:0 device type\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2f35ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>The secret to become successful in Robotics industry is 3 things. This is the opinion of many professionals in the field. They are:\n",
      "1. Passion\n",
      "2. Hard work\n",
      "3. Networking\n",
      "These are the three key elements that will help you become successful in the Robotics industry. Passion will drive you to learn and improve continuously. Hard work will help you to apply your knowledge and skills to achieve your goals. Networking will help you to find opportunities, get advice and support from experienced professionals.\n",
      "The Robotics industry is a highly competitive field, and it requires a lot of dedication and perseverance to succeed. However, with the right mindset and approach, it is possible to achieve great things and make a meaningful impact in this field.\n",
      "Here are some additional\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode(generated_ids)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d41bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Can you elaborate on those 3 things?  I'm not sure I fully understand what you're saying.\n",
      "1. \"You're trying to find the most efficient way to get from point A to point B.\"  I think I understand this part.  I'm trying to find the most efficient way to get from my current understanding of the universe to a deeper understanding of the universe.\n",
      "2. \"You're trying to find a new way to see the universe.\"  I'm not sure what this means.  Are you saying that I'm trying to find a new perspective or a new way of understanding the universe that isn't based on my current knowledge or experience?\n",
      "3. \"You're trying to find a new way to describe the universe.\"  I think I understand this part.  I'm trying to find new words or concepts to describe the universe, or to describe the things that I experience in the universe.\n",
      "\n",
      "I think I understand what you're saying, but I'd like\n"
     ]
    }
   ],
   "source": [
    "model_inputs = tokenizer([\"Can you elaborate on those 3 things? \"], return_tensors=\"pt\").to(\"cuda\")\n",
    "generated_ids = model.generate(**model_inputs, max_length=200)\n",
    "print(tokenizer.batch_decode(generated_ids)[0])\n",
    "'''\n",
    " Took 6 mins and 19 seconds to generate and output for max length 200 on my 8Gb Vram : RTX 4070 laptop\n",
    " Moreover, it consumed all of my Vram when I just load the model in Cuda:0 device type\n",
    " It didnot cache my previous inputs so it started from scratch every time.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6794de69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>what is your name? i am a student\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n Took 6 mins and 19 seconds to generate and output for max length 200 on my 8Gb Vram : RTX 4070 laptop\\n Moreover, it consumed all of my Vram when I just load the model in Cuda:0 device type\\n It didnot cache my previous inputs so it started from scratch every time.\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs = tokenizer([\"what is your name?\"], return_tensors=\"pt\").to(\"cuda\")\n",
    "generated_ids = model.generate(**model_inputs, max_length=10)\n",
    "print(tokenizer.batch_decode(generated_ids)[0])\n",
    "'''\n",
    " Took 7 seconds to generate and output for max length 10 on my 8Gb Vram : RTX 4070 laptop\n",
    " Moreover, it consumed all of my Vram when I just load the model in Cuda:0 device type\n",
    " It didnot cache my previous inputs so it started from scratch every time.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b7334a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>The secret of staying happy is to find the joy in everyday moments. It’s a mindset,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n Took 7 seconds to generate and output for max length 10 on my 8Gb Vram : RTX 4070 laptop\\n Moreover, it consumed all of my Vram when I just load the model in Cuda:0 device type\\n It didnot cache my previous inputs so it started from scratch every time.\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs = tokenizer([\"The secret of staying happy is\"], return_tensors=\"pt\").to(\"cuda\")\n",
    "generated_ids = model.generate(**model_inputs, max_length=20)\n",
    "print(tokenizer.batch_decode(generated_ids)[0])\n",
    "'''\n",
    " Took 25 seconds to generate and output for max length 20 on my 8Gb Vram : RTX 4070 laptop\n",
    "\n",
    " Length of the response needed is directly proportional to the time taken to generate it.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c7745c",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "- The Pipeline class is the most convenient way to inference with a pretrained model. It supports many tasks such as text generation, image segmentation, automatic speech recognition, document question answering, and more.\n",
    "- Create a Pipeline object and select a task. By default, Pipeline downloads and caches a default pretrained model for a given task. Pass the model name to the model parameter to choose a specific model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fbba17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AI_TEXt_Generator\\llama\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  9.29it/s]\n",
      "Device set to use cuda\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"The secret to get hired in FAANG companies is not a secret. However, the path to get hired in these companies is often misunderstood. Here's what it takes to get hired in FAANG companies.\\nThe FAANG companies (Facebook, Apple, Amazon, Netflix, and Google) are known for their rigorous hiring processes, and getting hired in these companies is often a challenging and competitive process. However, the secret to getting hired in these companies is not a secret. Here's what it takes to get hired in FAANG companies:\\n1. **Develop in-demand skills**: FAANG companies are constantly looking for professionals with in-demand skills, such as machine learning, artificial intelligence, cloud computing, data science, cybersecurity, and software engineering. Stay up-to-date with industry trends and develop skills that are in high demand.\\n2. **Gain relevant experience**: FAANG companies prefer candidates with relevant work experience, especially in the tech industry. Internships, co-op programs, and entry-level positions can provide valuable experience and help you build a strong network.\\n3. **Build a strong network**: Networking is key to getting hired in FAANG companies. Attend industry events, join professional organizations, and connect with professionals in your desired field on LinkedIn. Building relationships with people who work in FAANG companies can provide valuable insights and referrals.\\n4\"}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipeline = pipeline(\"text-generation\", model=\"./SavedModels/llama3.1-Instruct\", device=\"cuda\")\n",
    "pipeline(\"The secret to get hired in FAANG companies is\", max_length=10, truncation=True)\n",
    "\n",
    "'''\n",
    " Took 9 mins and 3  seconds to generate and output for max length 256 on my 8Gb Vram : RTX 4070 laptop\n",
    "\n",
    " Length of the response needed is directly proportional to the time taken to generate it.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b2d684",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 111.85it/s]\n",
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"The secret is out: the best way to get a good night's sleep is not to try to get a good night's sleep.\\nIn other words, the more you worry about getting a good night's sleep, the less likely you are to actually get one. This is because sleep is a complex process that involves the interplay of multiple factors, including your brain's ability to regulate its own sleep-wake cycle, your physical environment, and your mental state.\\nWhen you try to force yourself to sleep, you can create a kind of self-fulfilling prophecy that makes it harder to fall asleep. This is because the more you focus on falling asleep, the more anxious and alert you become, which can actually interfere with your ability to relax and fall asleep.\\nSo, instead of trying to get a good night's sleep, try this: stop trying to get a good night's sleep. Yes, you read that right. Just let go of the expectation that you need to sleep well, and instead focus on creating a relaxing and calming environment that allows your body to naturally drift off to sleep.\\nHere are some tips to help you do just that:\\n1. Create a sleep-conducive environment: Make your bedroom a sleep sanctuary by keeping it cool, dark, and quiet. Consider\"}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipeline = pipeline(\"text-generation\", model=\"./SavedModels/llama3.1-Instruct\", device=\"cpu\")\n",
    "pipeline(\"The secret is\", max_length=10)\n",
    "\n",
    "'''\n",
    " Took 2 mins and 43  seconds to generate and output for max length 256 on my 32 ram laptop when ran in CPU mode\n",
    "\n",
    " Length of the response needed is directly proportional to the time taken to generate it.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581897f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
